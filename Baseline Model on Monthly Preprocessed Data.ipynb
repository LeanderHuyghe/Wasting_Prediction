{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\"\"\"------------SECTION IMPORTS---------------------\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "'''------------SECTION USER VARIABLES--------------'''\n",
    "# Define the path to your datafolder below\n",
    "your_datapath = '../data/'\n",
    "\n",
    "# Define search space for number of trees in random forest and depth of trees\n",
    "num_trees_min = 64\n",
    "num_trees_max = 128\n",
    "\n",
    "depth_min = 2\n",
    "depth_max = 7"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Useful functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Function that returns every possible subset (except the empty set) of the input list l\n",
    "def subsets(l: object) -> object:\n",
    "    subset_list = []\n",
    "    for i in range(len(l) + 1):\n",
    "        for j in range(i):\n",
    "            subset_list.append(l[j: i])\n",
    "    return subset_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import data\n",
    "NaN data needs to be dropped for RF to work\n",
    "1. I drop the columns which have NaN values which is all the new data\n",
    "2. I create a new dataframe which has no columns containing NaN values\n",
    "3. Copy the `next_prevalence` column to this dataframe and once again drop rows containing NaN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/no_missings_mon.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0        date       district  prevalence  next_prevalence  \\\n0         445  2018-01-01  Qansax Dheere      0.2886           0.2886   \n1         446  2018-01-01          Jilib      0.3926           0.3926   \n2         447  2018-01-01     Wanla Weyn      0.3510           0.3510   \n3         448  2018-01-01      Qoryooley      0.3510           0.3510   \n4         449  2018-01-01          Xudur      0.4862           0.4862   \n\n   prevalence_6lag  ndvi   ipc  population  month  district_encoded  increase  \n0           0.3588  0.15  0.09  123818.085      1                63      True  \n1           0.4316  0.22  0.09  103739.085      1                51      True  \n2           0.3692  0.14  0.00  227667.915      1                74      True  \n3           0.3692  0.20  0.09  196309.485      1                65      True  \n4           0.4862  0.11  0.14  113853.105      1                77      True  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>date</th>\n      <th>district</th>\n      <th>prevalence</th>\n      <th>next_prevalence</th>\n      <th>prevalence_6lag</th>\n      <th>ndvi</th>\n      <th>ipc</th>\n      <th>population</th>\n      <th>month</th>\n      <th>district_encoded</th>\n      <th>increase</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>445</td>\n      <td>2018-01-01</td>\n      <td>Qansax Dheere</td>\n      <td>0.2886</td>\n      <td>0.2886</td>\n      <td>0.3588</td>\n      <td>0.15</td>\n      <td>0.09</td>\n      <td>123818.085</td>\n      <td>1</td>\n      <td>63</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>446</td>\n      <td>2018-01-01</td>\n      <td>Jilib</td>\n      <td>0.3926</td>\n      <td>0.3926</td>\n      <td>0.4316</td>\n      <td>0.22</td>\n      <td>0.09</td>\n      <td>103739.085</td>\n      <td>1</td>\n      <td>51</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>447</td>\n      <td>2018-01-01</td>\n      <td>Wanla Weyn</td>\n      <td>0.3510</td>\n      <td>0.3510</td>\n      <td>0.3692</td>\n      <td>0.14</td>\n      <td>0.00</td>\n      <td>227667.915</td>\n      <td>1</td>\n      <td>74</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>448</td>\n      <td>2018-01-01</td>\n      <td>Qoryooley</td>\n      <td>0.3510</td>\n      <td>0.3510</td>\n      <td>0.3692</td>\n      <td>0.20</td>\n      <td>0.09</td>\n      <td>196309.485</td>\n      <td>1</td>\n      <td>65</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>449</td>\n      <td>2018-01-01</td>\n      <td>Xudur</td>\n      <td>0.4862</td>\n      <td>0.4862</td>\n      <td>0.4862</td>\n      <td>0.11</td>\n      <td>0.14</td>\n      <td>113853.105</td>\n      <td>1</td>\n      <td>77</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define X and y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''------------SECTION RANDOM FOREST CROSS VALIDATION--------------'''\n",
    "# WARNING: this process can take some time, since there are a lot of hyperparameters to investigate. The search space can be manually reduced to speed up the process.\n",
    "\n",
    "# Create empty list to store model scores\n",
    "parameter_scores = []\n",
    "\n",
    "# Define target and explanatory variables\n",
    "X = df.select_dtypes(exclude=[\"category\",\"object\"]).drop([\"increase\", \"prevalence\", \"next_prevalence\"],axis=1)\n",
    "y = df['next_prevalence'].values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First 5 observations\n",
    "train_split = 345"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RF-CV"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for num_trees in tqdm(range(num_trees_min, num_trees_max)):\n",
    "\n",
    "    for depth in range(depth_min, depth_max):\n",
    "\n",
    "        # Investigate every subset of explanatory variables\n",
    "        # noinspection PyTypeChecker\n",
    "        for features in subsets(X.columns):\n",
    "            # First CV split. The 207 refers to the first 3 observations for the 69 districts in the data.\n",
    "            #\n",
    "            Xtrain = X[:207][features].copy().values\n",
    "            ytrain = y[:207]\n",
    "            Xtest = X[207:][features].copy().values\n",
    "            ytest = y[207:]\n",
    "\n",
    "            # Create a RandomForestRegressor with the selected hyperparameters and random state 0.\n",
    "            clf = RandomForestRegressor(n_estimators=num_trees, max_depth=depth, random_state=0, n_jobs=-1)\n",
    "\n",
    "            # Fit to the training data\n",
    "            clf.fit(Xtrain, ytrain)\n",
    "\n",
    "            # Make a prediction on the test data\n",
    "            predictions = clf.predict(Xtest)\n",
    "\n",
    "            # Calculate mean absolute error\n",
    "            MAE1 = mean_absolute_error(ytest, predictions)\n",
    "\n",
    "            # Second CV split. The 276 refers to the first 4 observations for the 69 districts in the data.\n",
    "            Xtrain = X[:276][features].copy().values\n",
    "            ytrain = y[:276]\n",
    "            Xtest = X[276:][features].copy().values\n",
    "            ytest = y[276:]\n",
    "\n",
    "            # Create a RandomForestRegressor with the selected hyperparameters and random state 0.\n",
    "            clf = RandomForestRegressor(n_estimators=num_trees, max_depth=depth, random_state=0, n_jobs=-1)\n",
    "\n",
    "            # Fit to the training data\n",
    "            clf.fit(Xtrain, ytrain)\n",
    "\n",
    "            # Make a prediction on the test data\n",
    "            predictions = clf.predict(Xtest)\n",
    "\n",
    "            # Calculate mean absolute error\n",
    "            MAE2 = mean_absolute_error(ytest, predictions)\n",
    "\n",
    "            # Calculate the mean MAE over the two folds\n",
    "            mean_MAE = (MAE1 + MAE2) / 2\n",
    "\n",
    "            # Store the mean MAE together with the used hyperparameters in list\n",
    "            parameter_scores.append((mean_MAE, num_trees, depth, features))\n",
    "\n",
    "# Sort the models based on score and retrieve the hyperparameters of the best model\n",
    "parameter_scores.sort(key=lambda x: x[0])\n",
    "best_model_score = parameter_scores[0][0]\n",
    "best_model_trees = parameter_scores[0][1]\n",
    "best_model_depth = parameter_scores[0][2]\n",
    "best_model_columns = list(parameter_scores[0][3])\n",
    "\n",
    "'''------------SECTION FINAL EVALUATION--------------'''\n",
    "X = df[best_model_columns].values\n",
    "y = df['next_prevalence'].values\n",
    "\n",
    "# If there is only one explanatory variable, the values need to be reshaped for the model\n",
    "if len(best_model_columns) == 1:\n",
    "    X = X.reshape(-1, 1)\n",
    "\n",
    "# Perform evaluation on full data\n",
    "Xtrain = X[:train_split]\n",
    "ytrain = y[:train_split]\n",
    "Xtest = X[train_split:]\n",
    "ytest = y[train_split:]\n",
    "\n",
    "clf = RandomForestRegressor(n_estimators=best_model_trees, max_depth=best_model_depth, random_state=0, n_jobs=-1)\n",
    "clf.fit(Xtrain, ytrain)\n",
    "predictions = clf.predict(Xtest)\n",
    "\n",
    "len(predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate MAE\n",
    "MAE = mean_absolute_error(ytest, predictions)\n",
    "\n",
    "# Generate boolean values for increase or decrease in prevalence. 0 if next prevalence is smaller than current prevalence, 1 otherwise.\n",
    "increase = [0 if x < y else 1 for x in df.iloc[165:]['next_prevalence'] for y in df.iloc[165:]['GAM prevalence']]\n",
    "predicted_increase = [0 if x < y else 1 for x in predictions for y in df.iloc[165:]['GAM prevalence']]\n",
    "\n",
    "# Calculate accuracy of predicted boolean increase/decrease\n",
    "acc = accuracy_score(increase, predicted_increase)\n",
    "\n",
    "# Print model parameters\n",
    "print('no. of trees: ' + str(best_model_trees) + '\\nmax_depth: ' + str(best_model_depth) + '\\ncolumns: ' + str(\n",
    "    best_model_columns))\n",
    "\n",
    "# Print model scores\n",
    "print(MAE, acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filename = 'baseline_monthly_model.joblib'\n",
    "joblib.dump(clf, filename)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from helper_metrics import make_confusion_matrix, roc_curve_gen, calculate_results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from helper_metrics import make_confusion_matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "make_confusion_matrix(increase, predicted_increase,figsize=(6,6),classes=[\"Increase\", \"Decrease\"],cmap=plt.cm.Greens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "roc_curve_gen(increase, predicted_increase)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "calculate_results(increase, predicted_increase, average=\"weighted\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
