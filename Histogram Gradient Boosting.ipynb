{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from helper_metrics import count_missing_district, count_missing_district_total\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.experimental    import enable_iterative_imputer\n",
    "from sklearn.impute          import IterativeImputer\n",
    "from sklearn.experimental    import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble        import HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics         import mean_absolute_error, accuracy_score\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "           date     district  total population  Under-Five Population  \\\n0    2017-07-01  Adan Yabaal       65262.96000            13052.59200   \n1    2017-07-01      Lughaye       70268.22000            14053.64400   \n2    2017-07-01    Buuhoodle       71317.71000            14263.54200   \n3    2017-07-01         Luuq      100476.76500            20095.35300   \n4    2017-07-01     Burtinle      112734.27000            22546.85400   \n..          ...          ...               ...                    ...   \n657  2021-07-01     Jariiban               NaN            32671.60000   \n658  2021-07-01      Caluula               NaN            16168.60000   \n659  2021-07-01    Qoryooley               NaN            25309.00087   \n660  2021-07-01         Baki               NaN            11187.80000   \n661  2021-07-01    Jalalaqsi               NaN            10831.40000   \n\n            GAM        MAM        SAM  GAM Prevalence  SAM Prevalence  \\\n0    4819.01697 3733.04131 1085.97565         0.36920         0.08320   \n1    5334.76326 4220.30929 1114.45397         0.37960         0.07930   \n2    4858.16241 3652.89311 1205.26930         0.34060         0.08450   \n3    8673.15435 7366.95641 1306.19795         0.43160         0.06500   \n4   10200.19675 8500.16396 1700.03279         0.45240         0.07540   \n..          ...        ...        ...             ...             ...   \n657 10890.00000        NaN 1430.00000         0.33332         0.04377   \n658  5560.00000        NaN  870.00000         0.34388         0.05381   \n659 11420.00000        NaN 2160.00000         0.45122         0.08535   \n660  3470.00000        NaN  640.00000         0.31016         0.05721   \n661  4420.00000        NaN  840.00000         0.40807         0.07755   \n\n     phase3plus_perc_x  ...  Total alarms  n_conflict_total  Average of centy  \\\n0              0.18000  ...       2.16667               NaN           3.54944   \n1              0.36000  ...       2.66667           1.00000          10.64738   \n2              0.37000  ...       2.33333           2.50000           8.46016   \n3              0.21000  ...       7.83333           1.50000           3.79293   \n4              0.22000  ...       3.66667               NaN           7.80220   \n..                 ...  ...           ...               ...               ...   \n657            0.19000  ...       2.16667               NaN           7.16378   \n658            0.16000  ...       3.16667           1.00000          11.66822   \n659            0.08000  ...       6.16667           3.50000           1.93456   \n660            0.37000  ...       0.66667           1.00000          10.28566   \n661            0.10000  ...       4.16667           3.33333           3.40329   \n\n     Average of centx  prevalence_6lag  next_prevalence  month  increase  \\\n0            46.54467              NaN          0.35100      7     False   \n1            43.57812              NaN          0.16900      7     False   \n2            46.66129              NaN          0.20280      7     False   \n3            42.69760              NaN          0.39260      7     False   \n4            48.39912              NaN          0.37960      7     False   \n..                ...              ...              ...    ...       ...   \n657          48.99860          0.34857              NaN      7       NaN   \n658          50.79402          0.35925              NaN      7       NaN   \n659          44.44943          0.45939              NaN      7       NaN   \n660          43.73210          0.22769              NaN      7       NaN   \n661          45.61273          0.19471              NaN      7       NaN   \n\n     increase_numeric  district_encoded  \n0            -0.01820                 0  \n1            -0.21060                58  \n2            -0.13780                23  \n3            -0.03900                59  \n4            -0.07280                22  \n..                ...               ...  \n657               NaN                50  \n658               NaN                28  \n659               NaN                65  \n660               NaN                 7  \n661               NaN                48  \n\n[662 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>district</th>\n      <th>total population</th>\n      <th>Under-Five Population</th>\n      <th>GAM</th>\n      <th>MAM</th>\n      <th>SAM</th>\n      <th>GAM Prevalence</th>\n      <th>SAM Prevalence</th>\n      <th>phase3plus_perc_x</th>\n      <th>...</th>\n      <th>Total alarms</th>\n      <th>n_conflict_total</th>\n      <th>Average of centy</th>\n      <th>Average of centx</th>\n      <th>prevalence_6lag</th>\n      <th>next_prevalence</th>\n      <th>month</th>\n      <th>increase</th>\n      <th>increase_numeric</th>\n      <th>district_encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-07-01</td>\n      <td>Adan Yabaal</td>\n      <td>65262.96000</td>\n      <td>13052.59200</td>\n      <td>4819.01697</td>\n      <td>3733.04131</td>\n      <td>1085.97565</td>\n      <td>0.36920</td>\n      <td>0.08320</td>\n      <td>0.18000</td>\n      <td>...</td>\n      <td>2.16667</td>\n      <td>NaN</td>\n      <td>3.54944</td>\n      <td>46.54467</td>\n      <td>NaN</td>\n      <td>0.35100</td>\n      <td>7</td>\n      <td>False</td>\n      <td>-0.01820</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-07-01</td>\n      <td>Lughaye</td>\n      <td>70268.22000</td>\n      <td>14053.64400</td>\n      <td>5334.76326</td>\n      <td>4220.30929</td>\n      <td>1114.45397</td>\n      <td>0.37960</td>\n      <td>0.07930</td>\n      <td>0.36000</td>\n      <td>...</td>\n      <td>2.66667</td>\n      <td>1.00000</td>\n      <td>10.64738</td>\n      <td>43.57812</td>\n      <td>NaN</td>\n      <td>0.16900</td>\n      <td>7</td>\n      <td>False</td>\n      <td>-0.21060</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017-07-01</td>\n      <td>Buuhoodle</td>\n      <td>71317.71000</td>\n      <td>14263.54200</td>\n      <td>4858.16241</td>\n      <td>3652.89311</td>\n      <td>1205.26930</td>\n      <td>0.34060</td>\n      <td>0.08450</td>\n      <td>0.37000</td>\n      <td>...</td>\n      <td>2.33333</td>\n      <td>2.50000</td>\n      <td>8.46016</td>\n      <td>46.66129</td>\n      <td>NaN</td>\n      <td>0.20280</td>\n      <td>7</td>\n      <td>False</td>\n      <td>-0.13780</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017-07-01</td>\n      <td>Luuq</td>\n      <td>100476.76500</td>\n      <td>20095.35300</td>\n      <td>8673.15435</td>\n      <td>7366.95641</td>\n      <td>1306.19795</td>\n      <td>0.43160</td>\n      <td>0.06500</td>\n      <td>0.21000</td>\n      <td>...</td>\n      <td>7.83333</td>\n      <td>1.50000</td>\n      <td>3.79293</td>\n      <td>42.69760</td>\n      <td>NaN</td>\n      <td>0.39260</td>\n      <td>7</td>\n      <td>False</td>\n      <td>-0.03900</td>\n      <td>59</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2017-07-01</td>\n      <td>Burtinle</td>\n      <td>112734.27000</td>\n      <td>22546.85400</td>\n      <td>10200.19675</td>\n      <td>8500.16396</td>\n      <td>1700.03279</td>\n      <td>0.45240</td>\n      <td>0.07540</td>\n      <td>0.22000</td>\n      <td>...</td>\n      <td>3.66667</td>\n      <td>NaN</td>\n      <td>7.80220</td>\n      <td>48.39912</td>\n      <td>NaN</td>\n      <td>0.37960</td>\n      <td>7</td>\n      <td>False</td>\n      <td>-0.07280</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>657</th>\n      <td>2021-07-01</td>\n      <td>Jariiban</td>\n      <td>NaN</td>\n      <td>32671.60000</td>\n      <td>10890.00000</td>\n      <td>NaN</td>\n      <td>1430.00000</td>\n      <td>0.33332</td>\n      <td>0.04377</td>\n      <td>0.19000</td>\n      <td>...</td>\n      <td>2.16667</td>\n      <td>NaN</td>\n      <td>7.16378</td>\n      <td>48.99860</td>\n      <td>0.34857</td>\n      <td>NaN</td>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>658</th>\n      <td>2021-07-01</td>\n      <td>Caluula</td>\n      <td>NaN</td>\n      <td>16168.60000</td>\n      <td>5560.00000</td>\n      <td>NaN</td>\n      <td>870.00000</td>\n      <td>0.34388</td>\n      <td>0.05381</td>\n      <td>0.16000</td>\n      <td>...</td>\n      <td>3.16667</td>\n      <td>1.00000</td>\n      <td>11.66822</td>\n      <td>50.79402</td>\n      <td>0.35925</td>\n      <td>NaN</td>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>659</th>\n      <td>2021-07-01</td>\n      <td>Qoryooley</td>\n      <td>NaN</td>\n      <td>25309.00087</td>\n      <td>11420.00000</td>\n      <td>NaN</td>\n      <td>2160.00000</td>\n      <td>0.45122</td>\n      <td>0.08535</td>\n      <td>0.08000</td>\n      <td>...</td>\n      <td>6.16667</td>\n      <td>3.50000</td>\n      <td>1.93456</td>\n      <td>44.44943</td>\n      <td>0.45939</td>\n      <td>NaN</td>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>660</th>\n      <td>2021-07-01</td>\n      <td>Baki</td>\n      <td>NaN</td>\n      <td>11187.80000</td>\n      <td>3470.00000</td>\n      <td>NaN</td>\n      <td>640.00000</td>\n      <td>0.31016</td>\n      <td>0.05721</td>\n      <td>0.37000</td>\n      <td>...</td>\n      <td>0.66667</td>\n      <td>1.00000</td>\n      <td>10.28566</td>\n      <td>43.73210</td>\n      <td>0.22769</td>\n      <td>NaN</td>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>661</th>\n      <td>2021-07-01</td>\n      <td>Jalalaqsi</td>\n      <td>NaN</td>\n      <td>10831.40000</td>\n      <td>4420.00000</td>\n      <td>NaN</td>\n      <td>840.00000</td>\n      <td>0.40807</td>\n      <td>0.07755</td>\n      <td>0.10000</td>\n      <td>...</td>\n      <td>4.16667</td>\n      <td>3.33333</td>\n      <td>3.40329</td>\n      <td>45.61273</td>\n      <td>0.19471</td>\n      <td>NaN</td>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>48</td>\n    </tr>\n  </tbody>\n</table>\n<p>662 rows × 23 columns</p>\n</div>"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/semiyearly_chosen_columns.csv\").iloc[:,1:]\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create train and test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "y = df.next_prevalence.dropna()\n",
    "X = df.select_dtypes(exclude=[\"object\", \"category\"]).iloc[:len(y)].drop(\"next_prevalence\", axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Subsets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "# Function that returns every possible subset (except the empty set) of the input list l\n",
    "def subsets(l: object) -> object:\n",
    "    subset_list = []\n",
    "    for i in range(len(l) + 1):\n",
    "        for j in range(i):\n",
    "            subset_list.append(l[j: i])\n",
    "    return subset_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cross Validation Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "365"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "73*5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [1:49:25<00:00, 198.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning 0.000 GB of training data: 0.006 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 12 leaves, max depth = 6, in 0.004s\n",
      "[2/100] 1 tree, 13 leaves, max depth = 6, in 0.004s\n",
      "[3/100] 1 tree, 13 leaves, max depth = 6, in 0.005s\n",
      "[4/100] 1 tree, 13 leaves, max depth = 6, in 0.004s\n",
      "[5/100] 1 tree, 13 leaves, max depth = 6, in 0.005s\n",
      "[6/100] 1 tree, 15 leaves, max depth = 6, in 0.006s\n",
      "[7/100] 1 tree, 12 leaves, max depth = 6, in 0.005s\n",
      "[8/100] 1 tree, 15 leaves, max depth = 5, in 0.004s\n",
      "[9/100] 1 tree, 14 leaves, max depth = 6, in 0.004s\n",
      "[10/100] 1 tree, 13 leaves, max depth = 6, in 0.004s\n",
      "[11/100] 1 tree, 14 leaves, max depth = 6, in 0.006s\n",
      "[12/100] 1 tree, 13 leaves, max depth = 6, in 0.004s\n",
      "[13/100] 1 tree, 14 leaves, max depth = 6, in 0.005s\n",
      "[14/100] 1 tree, 12 leaves, max depth = 6, in 0.005s\n",
      "[15/100] 1 tree, 9 leaves, max depth = 6, in 0.004s\n",
      "[16/100] 1 tree, 8 leaves, max depth = 6, in 0.004s\n",
      "[17/100] 1 tree, 13 leaves, max depth = 6, in 0.006s\n",
      "[18/100] 1 tree, 11 leaves, max depth = 6, in 0.004s\n",
      "[19/100] 1 tree, 11 leaves, max depth = 6, in 0.005s\n",
      "[20/100] 1 tree, 8 leaves, max depth = 6, in 0.004s\n",
      "[21/100] 1 tree, 14 leaves, max depth = 6, in 0.005s\n",
      "[22/100] 1 tree, 8 leaves, max depth = 6, in 0.003s\n",
      "[23/100] 1 tree, 15 leaves, max depth = 6, in 0.004s\n",
      "[24/100] 1 tree, 8 leaves, max depth = 6, in 0.004s\n",
      "[25/100] 1 tree, 15 leaves, max depth = 6, in 0.005s\n",
      "[26/100] 1 tree, 9 leaves, max depth = 6, in 0.005s\n",
      "[27/100] 1 tree, 10 leaves, max depth = 6, in 0.004s\n",
      "[28/100] 1 tree, 9 leaves, max depth = 6, in 0.004s\n",
      "[29/100] 1 tree, 15 leaves, max depth = 6, in 0.005s\n",
      "[30/100] 1 tree, 7 leaves, max depth = 6, in 0.004s\n",
      "[31/100] 1 tree, 16 leaves, max depth = 6, in 0.005s\n",
      "[32/100] 1 tree, 10 leaves, max depth = 6, in 0.003s\n",
      "[33/100] 1 tree, 8 leaves, max depth = 6, in 0.025s\n",
      "[34/100] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tree, 13 leaves, max depth = 6, in 0.007s\n",
      "[35/100] 1 tree, 14 leaves, max depth = 6, in 0.005s\n",
      "[36/100] 1 tree, 8 leaves, max depth = 6, in 0.003s\n",
      "[37/100] 1 tree, 9 leaves, max depth = 6, in 0.004s\n",
      "[38/100] 1 tree, 9 leaves, max depth = 6, in 0.004s\n",
      "[39/100] 1 tree, 14 leaves, max depth = 6, in 0.005s\n",
      "[40/100] 1 tree, 7 leaves, max depth = 6, in 0.003s\n",
      "[41/100] 1 tree, 9 leaves, max depth = 6, in 0.003s\n",
      "[42/100] 1 tree, 13 leaves, max depth = 6, in 0.004s\n",
      "[43/100] 1 tree, 8 leaves, max depth = 6, in 0.004s\n",
      "[44/100] 1 tree, 15 leaves, max depth = 6, in 0.004s\n",
      "[45/100] 1 tree, 7 leaves, max depth = 6, in 0.003s\n",
      "[46/100] 1 tree, 12 leaves, max depth = 6, in 0.006s\n",
      "[47/100] 1 tree, 10 leaves, max depth = 6, in 0.005s\n",
      "[48/100] 1 tree, 12 leaves, max depth = 6, in 0.004s\n",
      "[49/100] 1 tree, 8 leaves, max depth = 6, in 0.004s\n",
      "[50/100] 1 tree, 13 leaves, max depth = 6, in 0.005s\n",
      "[51/100] 1 tree, 11 leaves, max depth = 6, in 0.004s\n",
      "[52/100] 1 tree, 8 leaves, max depth = 6, in 0.004s\n",
      "[53/100] 1 tree, 10 leaves, max depth = 6, in 0.004s\n",
      "[54/100] 1 tree, 9 leaves, max depth = 6, in 0.004s\n",
      "[55/100] 1 tree, 11 leaves, max depth = 6, in 0.004s\n",
      "[56/100] 1 tree, 12 leaves, max depth = 6, in 0.003s\n",
      "[57/100] 1 tree, 7 leaves, max depth = 6, in 0.004s\n",
      "[58/100] 1 tree, 11 leaves, max depth = 6, in 0.003s\n",
      "[59/100] 1 tree, 12 leaves, max depth = 6, in 0.004s\n",
      "[60/100] 1 tree, 8 leaves, max depth = 6, in 0.003s\n",
      "[61/100] 1 tree, 9 leaves, max depth = 6, in 0.004s\n",
      "[62/100] 1 tree, 9 leaves, max depth = 6, in 0.004s\n",
      "[63/100] 1 tree, 10 leaves, max depth = 6, in 0.005s\n",
      "[64/100] 1 tree, 9 leaves, max depth = 6, in 0.004s\n",
      "[65/100] 1 tree, 12 leaves, max depth = 6, in 0.005s\n",
      "[66/100] 1 tree, 11 leaves, max depth = 6, in 0.004s\n",
      "[67/100] 1 tree, 10 leaves, max depth = 6, in 0.004s\n",
      "[68/100] 1 tree, 10 leaves, max depth = 6, in 0.004s\n",
      "[69/100] 1 tree, 9 leaves, max depth = 6, in 0.003s\n",
      "[70/100] 1 tree, 11 leaves, max depth = 6, in 0.004s\n",
      "[71/100] 1 tree, 10 leaves, max depth = 6, in 0.004s\n",
      "[72/100] 1 tree, 8 leaves, max depth = 6, in 0.003s\n",
      "[73/100] 1 tree, 9 leaves, max depth = 6, in 0.003s\n",
      "[74/100] 1 tree, 12 leaves, max depth = 6, in 0.004s\n",
      "[75/100] 1 tree, 9 leaves, max depth = 6, in 0.004s\n",
      "[76/100] 1 tree, 8 leaves, max depth = 6, in 0.003s\n",
      "[77/100] 1 tree, 11 leaves, max depth = 6, in 0.004s\n",
      "[78/100] 1 tree, 7 leaves, max depth = 6, in 0.003s\n",
      "[79/100] 1 tree, 10 leaves, max depth = 6, in 0.004s\n",
      "[80/100] 1 tree, 8 leaves, max depth = 6, in 0.003s\n",
      "[81/100] 1 tree, 14 leaves, max depth = 6, in 0.004s\n",
      "[82/100] 1 tree, 11 leaves, max depth = 6, in 0.006s\n",
      "[83/100] 1 tree, 12 leaves, max depth = 6, in 0.006s\n",
      "[84/100] 1 tree, 9 leaves, max depth = 6, in 0.005s\n",
      "[85/100] 1 tree, 12 leaves, max depth = 6, in 0.005s\n",
      "[86/100] 1 tree, 11 leaves, max depth = 6, in 0.004s\n",
      "[87/100] 1 tree, 7 leaves, max depth = 6, in 0.003s\n",
      "[88/100] 1 tree, 9 leaves, max depth = 6, in 0.004s\n",
      "[89/100] 1 tree, 12 leaves, max depth = 6, in 0.004s\n",
      "[90/100] 1 tree, 11 leaves, max depth = 6, in 0.004s\n",
      "[91/100] 1 tree, 8 leaves, max depth = 6, in 0.003s\n",
      "[92/100] 1 tree, 10 leaves, max depth = 6, in 0.004s\n",
      "[93/100] 1 tree, 10 leaves, max depth = 6, in 0.004s\n",
      "[94/100] 1 tree, 12 leaves, max depth = 6, in 0.004s\n",
      "[95/100] 1 tree, 11 leaves, max depth = 6, in 0.004s\n",
      "[96/100] 1 tree, 12 leaves, max depth = 6, in 0.005s\n",
      "[97/100] 1 tree, 12 leaves, max depth = 6, in 0.005s\n",
      "[98/100] 1 tree, 9 leaves, max depth = 6, in 0.004s\n",
      "[99/100] 1 tree, 8 leaves, max depth = 6, in 0.003s\n",
      "[100/100] 1 tree, 10 leaves, max depth = 6, in 0.004s\n",
      "Fit 100 trees in 0.460 s, (1072 total leaves)\n",
      "Time spent computing histograms: 0.070s\n",
      "Time spent finding best splits:  0.035s\n",
      "Time spent applying splits:      0.082s\n",
      "Time spent predicting:           0.005s\n"
     ]
    }
   ],
   "source": [
    "# Define search space for number of trees in random forest and depth of trees\n",
    "num_trees_min = 31\n",
    "num_trees_max = 64\n",
    "\n",
    "depth_min = 2\n",
    "depth_max = 7\n",
    "\n",
    "parameter_scores = []\n",
    "\n",
    "for num_trees in tqdm(range(num_trees_min, num_trees_max)):\n",
    "\n",
    "    for depth in range(depth_min, depth_max):\n",
    "\n",
    "        # Investigate every subset of explanatory variables\n",
    "        for features in subsets(X.columns):\n",
    "            # First CV split. The 99 refers to the first 3 observations for the 33 districts in the data.\n",
    "            Xtrain = X[:219][features].copy().values\n",
    "            ytrain = y[:219]\n",
    "            Xtest = X[219:292][features].copy().values\n",
    "            ytest = y[219:292]\n",
    "\n",
    "            # Create a RandomForestRegressor with the selected hyperparameters and random state 0.\n",
    "            clf = HistGradientBoostingRegressor(max_leaf_nodes=num_trees, max_depth=depth, random_state=0)\n",
    "\n",
    "            # Fit to the training data\n",
    "            clf.fit(Xtrain, ytrain)\n",
    "\n",
    "            # Make a prediction on the test data\n",
    "            predictions = clf.predict(Xtest)\n",
    "\n",
    "            # Calculate mean absolute error\n",
    "            MAE1 = mean_absolute_error(ytest, predictions)\n",
    "\n",
    "            # Second CV split. The 132 refers to the first 4 observations for the 33 districts in the data.\n",
    "            Xtrain = X[:292][features].copy().values\n",
    "            ytrain = y[:292]\n",
    "            Xtest = X[292:365][features].copy().values\n",
    "            ytest = y[292:365]\n",
    "\n",
    "            # Create a RandomForestRegressor with the selected hyperparameters and random state 0.\n",
    "            clf = HistGradientBoostingRegressor(max_leaf_nodes=num_trees, max_depth=depth, random_state=0)\n",
    "\n",
    "            # Fit to the training data\n",
    "            clf.fit(Xtrain, ytrain)\n",
    "\n",
    "            # Make a prediction on the test data\n",
    "            predictions = clf.predict(Xtest)\n",
    "\n",
    "            # Calculate mean absolute error\n",
    "            MAE2 = mean_absolute_error(ytest, predictions)\n",
    "\n",
    "            # Calculate the mean MAE over the two folds\n",
    "            mean_MAE = (MAE1 + MAE2) / 2\n",
    "\n",
    "            # Store the mean MAE together with the used hyperparameters in list\n",
    "            parameter_scores.append((mean_MAE, num_trees, depth, features))\n",
    "\n",
    "# Sort the models based on score and retrieve the hyperparameters of the best model\n",
    "parameter_scores.sort(key=lambda x: x[0])\n",
    "best_model_score = parameter_scores[0][0]\n",
    "best_model_trees = parameter_scores[0][1]\n",
    "best_model_depth = parameter_scores[0][2]\n",
    "best_model_columns = list(parameter_scores[0][3])\n",
    "\n",
    "'''------------SECTION FINAL EVALUATION--------------'''\n",
    "y = df['next_prevalence'].values\n",
    "X = df[best_model_columns].values\n",
    "\n",
    "# If there is only one explanatory variable, the values need to be reshaped for the model\n",
    "if len(best_model_columns) == 1:\n",
    "    X = X.reshape(-1, 1)\n",
    "\n",
    "# Peform evaluation on full data\n",
    "Xtrain = X[:365]\n",
    "ytrain = y[:365]\n",
    "Xtest = X[365:]\n",
    "ytest = y[365:]\n",
    "\n",
    "clf = HistGradientBoostingRegressor(max_leaf_nodes=best_model_trees, max_depth=best_model_depth, random_state=0, verbose=1)\n",
    "clf.fit(Xtrain, ytrain)\n",
    "predictions = clf.predict(Xtest)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "data": {
      "text/plain": "(88209, 88209)"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate MAE\n",
    "y_true = pd.Series(ytest[:-73]).drop(69)\n",
    "y_pred = pd.Series(predictions[:-73]).drop(69)\n",
    "#MAE = mean_absolute_error(ytest, predictions)\n",
    "MAE = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# Generate boolean values for increase or decrease in prevalence. 0 if next prevalence is smaller than current prevalence, 1 otherwise.\n",
    "increase = [0 if x < y else 1 for x in df.iloc[365:]['next_prevalence'] for y in df.iloc[365:]['GAM Prevalence']]\n",
    "predicted_increase = [0 if x < y else 1 for x in predictions for y in df.iloc[365:]['GAM Prevalence']]\n",
    "\n",
    "len(increase), len(predicted_increase)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of trees: 31\n",
      "max_depth: 6\n",
      "columns: ['total population', 'Under-Five Population', 'GAM', 'MAM', 'SAM', 'GAM Prevalence', 'SAM Prevalence', 'phase3plus_perc_x', 'rainfall', 'ndvi_score', 'Price of water', 'Total alarms', 'n_conflict_total', 'Average of centy', 'Average of centx', 'prevalence_6lag', 'month', 'increase_numeric']\n",
      "0.021789393055805115 0.8224217483476742\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy of predicted boolean increase/decrease\n",
    "acc = accuracy_score(increase, predicted_increase)\n",
    "\n",
    "# Print model parameters\n",
    "print('no. of trees: ' + str(best_model_trees) + '\\nmax_depth: ' + str(best_model_depth) + '\\ncolumns: ' + str(\n",
    "    best_model_columns))\n",
    "\n",
    "# Print model scores\n",
    "print(MAE, acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
